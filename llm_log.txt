nohup: ignoring input
INFO 01-06 16:33:22 [__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=2107357)[0;0m INFO 01-06 16:33:24 [api_server.py:1896] vLLM API server version 0.10.2rc1
[1;36m(APIServer pid=2107357)[0;0m INFO 01-06 16:33:24 [utils.py:328] non-default args: {'model_tag': '/data/models/Qwen-Qwen3-32B', 'port': 8888, 'api_key': ['00000000'], 'model': '/data/models/Qwen-Qwen3-32B'}
[1;36m(APIServer pid=2107357)[0;0m INFO 01-06 16:33:31 [__init__.py:744] Resolved architecture: Qwen3ForCausalLM
[1;36m(APIServer pid=2107357)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=2107357)[0;0m INFO 01-06 16:33:31 [__init__.py:1798] Using max model len 40960
[1;36m(APIServer pid=2107357)[0;0m INFO 01-06 16:33:31 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 01-06 16:33:35 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2108089)[0;0m INFO 01-06 16:33:38 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2108089)[0;0m INFO 01-06 16:33:38 [core.py:76] Initializing a V1 LLM engine (v0.10.2rc1) with config: model='/data/models/Qwen-Qwen3-32B', speculative_config=None, tokenizer='/data/models/Qwen-Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/models/Qwen-Qwen3-32B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[W106 16:33:40.927338073 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2108089)[0;0m INFO 01-06 16:33:40 [parallel_state.py:1164] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2108089)[0;0m WARNING 01-06 16:33:40 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2108089)[0;0m INFO 01-06 16:33:40 [gpu_model_runner.py:2178] Starting to load model /data/models/Qwen-Qwen3-32B...
[1;36m(EngineCore_DP0 pid=2108089)[0;0m INFO 01-06 16:33:40 [gpu_model_runner.py:2210] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2108089)[0;0m INFO 01-06 16:33:40 [cuda.py:338] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:01<00:22,  1.51s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:03<00:21,  1.56s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  19% Completed | 3/16 [00:05<00:23,  1.80s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  25% Completed | 4/16 [00:06<00:19,  1.62s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  31% Completed | 5/16 [00:08<00:17,  1.58s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  38% Completed | 6/16 [00:09<00:16,  1.61s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  44% Completed | 7/16 [00:11<00:14,  1.61s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  50% Completed | 8/16 [00:12<00:12,  1.62s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  56% Completed | 9/16 [00:14<00:11,  1.63s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  62% Completed | 10/16 [00:16<00:09,  1.61s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  69% Completed | 11/16 [00:17<00:07,  1.60s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  75% Completed | 12/16 [00:19<00:06,  1.64s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  81% Completed | 13/16 [00:21<00:04,  1.65s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  88% Completed | 14/16 [00:22<00:03,  1.63s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards:  94% Completed | 15/16 [00:24<00:01,  1.63s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:25<00:00,  1.62s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:25<00:00,  1.62s/it]
[1;36m(EngineCore_DP0 pid=2108089)[0;0m 
[1;36m(EngineCore_DP0 pid=2108089)[0;0m INFO 01-06 16:34:07 [default_loader.py:266] Loading weights took 26.04 seconds
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 82, in __init__
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     self._init_executor()
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 49, in _init_executor
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     self.collective_rpc("load_model")
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/utils/__init__.py", line 3045, in run_method
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2211, in load_model
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     self.load_weights(model, model_config)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/default_loader.py", line 275, in load_weights
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718]     raise ValueError("Following weights were not initialized from "
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ERROR 01-06 16:34:07 [core.py:718] ValueError: Following weights were not initialized from checkpoint: {'model.layers.22.self_attn.q_norm.weight', 'model.layers.21.mlp.gate_up_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.self_attn.qkv_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.19.mlp.gate_up_proj.weight', 'model.layers.20.self_attn.q_norm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.self_attn.k_norm.weight', 'model.layers.22.self_attn.k_norm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.22.self_attn.qkv_proj.weight', 'model.layers.21.self_attn.k_norm.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.20.mlp.gate_up_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.19.self_attn.qkv_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.self_attn.q_norm.weight', 'model.layers.19.self_attn.q_norm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.21.self_attn.qkv_proj.weight', 'model.layers.19.self_attn.k_norm.weight'}
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=2108089)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 722, in run_engine_core
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 82, in __init__
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 49, in _init_executor
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     self.collective_rpc("load_model")
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/utils/__init__.py", line 3045, in run_method
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 213, in load_model
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2211, in load_model
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     self.load_weights(model, model_config)
[1;36m(EngineCore_DP0 pid=2108089)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/default_loader.py", line 275, in load_weights
[1;36m(EngineCore_DP0 pid=2108089)[0;0m     raise ValueError("Following weights were not initialized from "
[1;36m(EngineCore_DP0 pid=2108089)[0;0m ValueError: Following weights were not initialized from checkpoint: {'model.layers.22.self_attn.q_norm.weight', 'model.layers.21.mlp.gate_up_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.self_attn.qkv_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.19.mlp.gate_up_proj.weight', 'model.layers.20.self_attn.q_norm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.self_attn.k_norm.weight', 'model.layers.22.self_attn.k_norm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.22.self_attn.qkv_proj.weight', 'model.layers.21.self_attn.k_norm.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.20.mlp.gate_up_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.19.self_attn.qkv_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.self_attn.q_norm.weight', 'model.layers.19.self_attn.q_norm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.21.self_attn.qkv_proj.weight', 'model.layers.19.self_attn.k_norm.weight'}
[rank0]:[W106 16:34:07.499243711 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(APIServer pid=2107357)[0;0m Traceback (most recent call last):
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/bin/vllm", line 7, in <module>
[1;36m(APIServer pid=2107357)[0;0m     sys.exit(main())
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/cli/main.py", line 54, in main
[1;36m(APIServer pid=2107357)[0;0m     args.dispatch_function(args)
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/cli/serve.py", line 50, in cmd
[1;36m(APIServer pid=2107357)[0;0m     uvloop.run(run_server(args))
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/uvloop/__init__.py", line 82, in run
[1;36m(APIServer pid=2107357)[0;0m     return loop.run_until_complete(wrapper())
[1;36m(APIServer pid=2107357)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/uvloop/__init__.py", line 61, in wrapper
[1;36m(APIServer pid=2107357)[0;0m     return await main
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1941, in run_server
[1;36m(APIServer pid=2107357)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1961, in run_server_worker
[1;36m(APIServer pid=2107357)[0;0m     async with build_async_engine_client(
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/contextlib.py", line 199, in __aenter__
[1;36m(APIServer pid=2107357)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 179, in build_async_engine_client
[1;36m(APIServer pid=2107357)[0;0m     async with build_async_engine_client_from_engine_args(
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/contextlib.py", line 199, in __aenter__
[1;36m(APIServer pid=2107357)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 221, in build_async_engine_client_from_engine_args
[1;36m(APIServer pid=2107357)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/utils/__init__.py", line 1587, in inner
[1;36m(APIServer pid=2107357)[0;0m     return fn(*args, **kwargs)
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 205, in from_vllm_config
[1;36m(APIServer pid=2107357)[0;0m     return cls(
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[1;36m(APIServer pid=2107357)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
[1;36m(APIServer pid=2107357)[0;0m     return AsyncMPClient(*client_args)
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 767, in __init__
[1;36m(APIServer pid=2107357)[0;0m     super().__init__(
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 446, in __init__
[1;36m(APIServer pid=2107357)[0;0m     with launch_core_engines(vllm_config, executor_class,
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/contextlib.py", line 142, in __exit__
[1;36m(APIServer pid=2107357)[0;0m     next(self.gen)
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 729, in launch_core_engines
[1;36m(APIServer pid=2107357)[0;0m     wait_for_engine_startup(
[1;36m(APIServer pid=2107357)[0;0m   File "/home/fangly/.conda/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 782, in wait_for_engine_startup
[1;36m(APIServer pid=2107357)[0;0m     raise RuntimeError("Engine core initialization failed. "
[1;36m(APIServer pid=2107357)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
